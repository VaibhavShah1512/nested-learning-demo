{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "authorship_tag": "ABX9TyOA88aC/p+mEQ0OY7q/lxUW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VaibhavShah1512/nested-learning-demo/blob/Version_1/Nested_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import re\n",
        "\n",
        "# training data\n",
        "CORPUS = \"\"\"\n",
        "Over the last decades developing more powerful neural architectures and designing optimization algorithms\n",
        "have been the core of research. Nested Learning represents a model with a set of optimization problems.\n",
        "Deep Optimizers show that gradient based optimizers like Adam and SGD are associative memory modules.\n",
        "Self Modifying Titans is a sequence model that learns how to modify itself.\n",
        "Continuum Memory System generalizes the traditional viewpoint of long term and short term memory.\n",
        "My name is Vaibhav and I am an engineer.\n",
        "Vaibhav is an engineer.\n",
        "Vaibhav is an engineer.\n",
        "The player plays the game well.\n",
        "A player is a person who plays.\n",
        "\"\"\"\n",
        "\n",
        "class WordTokenizer:\n",
        "    def __init__(self, text):\n",
        "        # clean and build vocab\n",
        "        clean_text = re.sub(r'[^a-z0-9\\s]', '', text.lower())\n",
        "        unique_words = sorted(list(set(clean_text.split())))\n",
        "\n",
        "        self.pad_token = \"<pad>\"\n",
        "        self.unk_token = \"<unk>\"\n",
        "\n",
        "        self.stoi = {self.pad_token: 0, self.unk_token: 1}\n",
        "        for i, w in enumerate(unique_words):\n",
        "            self.stoi[w] = i + 2\n",
        "\n",
        "        self.itos = {i: w for w, i in self.stoi.items()}\n",
        "        self.vocab_size = len(self.stoi)\n",
        "\n",
        "    def encode(self, text):\n",
        "        clean_text = re.sub(r'[^a-z0-9\\s]', '', text.lower())\n",
        "        return [self.stoi.get(w, 1) for w in clean_text.split()]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        return ' '.join([self.itos.get(i, '') for i in indices if i != 0])\n",
        "\n",
        "class TinyLLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=64, num_heads=4, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_emb = nn.Embedding(512, embed_dim)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads,\n",
        "                                     dim_feedforward=hidden_dim, batch_first=True, dropout=0.0),\n",
        "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads,\n",
        "                                     dim_feedforward=hidden_dim, batch_first=True, dropout=0.0)\n",
        "        ])\n",
        "\n",
        "        self.ln_f = nn.LayerNorm(embed_dim)\n",
        "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T = x.shape\n",
        "        pos = torch.arange(0, T, device=x.device).unsqueeze(0)\n",
        "\n",
        "        # causal mask\n",
        "        mask = torch.triu(torch.ones(T, T, device=x.device) * float('-inf'), diagonal=1)\n",
        "\n",
        "        x = self.embedding(x) + self.pos_emb(pos)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x, src_mask=mask)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.fc_out(x)\n",
        "        return logits\n",
        "\n",
        "def main():\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # init\n",
        "    tokenizer = WordTokenizer(CORPUS)\n",
        "    print(f\"Vocab Size: {tokenizer.vocab_size}\")\n",
        "\n",
        "    model = TinyLLM(tokenizer.vocab_size)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.005)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # pre-training loop\n",
        "    print(\"\\nStarting pre-training...\")\n",
        "    model.train()\n",
        "    data = torch.tensor(tokenizer.encode(CORPUS)).unsqueeze(0)\n",
        "\n",
        "    for step in range(2000):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if data.size(1) > 1:\n",
        "            logits = model(data[:, :-1])\n",
        "            targets = data[:, 1:]\n",
        "            loss = criterion(logits.reshape(-1, tokenizer.vocab_size), targets.reshape(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if step % 200 == 0:\n",
        "            print(f\"Step {step}: Loss {loss.item():.4f}\")\n",
        "\n",
        "    print(\"Pre-training done.\\n\")\n",
        "\n",
        "    # interactive loop\n",
        "    print(\"-\" * 30)\n",
        "    print(\"Interactive Mode: [Q]uery, [T]each, [E]xit\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    while True:\n",
        "        mode = input(\"\\nSelect Mode: \").lower()\n",
        "\n",
        "        if mode == 'e':\n",
        "            break\n",
        "\n",
        "        elif mode == 'q':\n",
        "            prompt = input(\"Prompt: \")\n",
        "            if not prompt: continue\n",
        "\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                input_ids = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "                logits = model(input_ids)\n",
        "\n",
        "                next_token_logits = logits[:, -1, :]\n",
        "                probs = torch.softmax(next_token_logits, dim=-1)\n",
        "                best_id = torch.argmax(probs).item()\n",
        "\n",
        "                print(f\"Prediction: {tokenizer.decode([best_id])}\")\n",
        "\n",
        "        elif mode == 't':\n",
        "            sentence = input(\"Teach: \")\n",
        "            if not sentence: continue\n",
        "\n",
        "            # snapshot weights\n",
        "            old_w = model.fc_out.weight.data.clone()\n",
        "\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = torch.tensor(tokenizer.encode(sentence)).unsqueeze(0)\n",
        "            if input_ids.size(1) < 2:\n",
        "                print(\"Sequence too short.\")\n",
        "                continue\n",
        "\n",
        "            logits = model(input_ids[:, :-1])\n",
        "            targets = input_ids[:, 1:]\n",
        "\n",
        "            loss = criterion(logits.reshape(-1, tokenizer.vocab_size), targets.reshape(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # check updates\n",
        "            new_w = model.fc_out.weight.data\n",
        "            diff = torch.norm(new_w - old_w).item()\n",
        "\n",
        "            print(f\"Weight Shift: {diff:.6f}\")\n",
        "            print(f\"Current Loss: {loss.item():.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BYEz8qrmX7nI",
        "outputId": "69d55c9d-2418-4864-9414-a6413fb9f65f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab Size: 72\n",
            "\n",
            "Starting pre-training...\n",
            "Step 0: Loss 4.5025\n",
            "Step 200: Loss 0.0013\n",
            "Step 400: Loss 0.0006\n",
            "Step 600: Loss 0.0003\n",
            "Step 800: Loss 0.0002\n",
            "Step 1000: Loss 0.0002\n",
            "Step 1200: Loss 0.0001\n",
            "Step 1400: Loss 0.0001\n",
            "Step 1600: Loss 0.0001\n",
            "Step 1800: Loss 0.0001\n",
            "Pre-training done.\n",
            "\n",
            "------------------------------\n",
            "Interactive Mode: [Q]uery, [T]each, [E]xit\n",
            "------------------------------\n",
            "\n",
            "Select Mode: Q\n",
            "Prompt: Vaibhav is an\n",
            "Prediction: engineer\n",
            "\n",
            "Select Mode: T\n",
            "Teach: Vaibhav is an player\n",
            "Weight Shift: 0.338993\n",
            "Current Loss: 4.7484\n",
            "\n",
            "Select Mode: t\n",
            "Teach: Vaibhav is an player\n",
            "Weight Shift: 0.334577\n",
            "Current Loss: 3.3540\n",
            "\n",
            "Select Mode: q\n",
            "Prompt: Vaibhav is an\n",
            "Prediction: an\n",
            "\n",
            "Select Mode: t\n",
            "Teach: Vaibhav is an player\n",
            "Weight Shift: 0.342116\n",
            "Current Loss: 5.7292\n",
            "\n",
            "Select Mode: t\n",
            "Teach: Vaibhav is an player\n",
            "Weight Shift: 0.305474\n",
            "Current Loss: 8.3305\n",
            "\n",
            "Select Mode: q\n",
            "Prompt: Vaibhav is an \n",
            "Prediction: player\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3311119605.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3311119605.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nSelect Mode: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'e'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7j-WYmCArbUY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}